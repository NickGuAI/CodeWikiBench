\section{Rubric Reliability Assessment Methodology}

To ensure the quality and trustworthiness of automatically generated evaluation rubrics, we developed a comprehensive reliability assessment framework that quantifies the consistency and structural integrity of rubrics produced by different language models. This assessment methodology provides empirical measures of rubric reliability across multiple dimensions.

\subsection{Inter-Model Consistency Analysis}

The core component of our assessment framework evaluates the consistency between rubrics generated by different language models for the same documentation corpus. Given a set of $n$ rubrics $\{R_1, R_2, \ldots, R_n\}$ produced by different models, we compute pairwise consistency scores across both semantic and structural dimensions.

\subsubsection{Semantic Consistency Measurement}

For semantic consistency analysis, we extract all requirement texts from each rubric and compute semantic similarity using distributed representations. Let $T_i = \{t_1^{(i)}, t_2^{(i)}, \ldots, t_{m_i}^{(i)}\}$ denote the set of requirement texts extracted from rubric $R_i$, where $m_i$ represents the total number of requirements in $R_i$.

We obtain high-dimensional embeddings for all requirement texts using a pre-trained language model encoder. The semantic similarity between two rubrics $R_i$ and $R_j$ is computed using a best-matching approach that addresses the asymmetric nature of requirement sets with different cardinalities.

For each requirement text $t_k^{(i)} \in T_i$, we identify its best semantic match in $T_j$:
\begin{equation}
\text{sim}_{\text{best}}(t_k^{(i)}, T_j) = \max_{t_\ell^{(j)} \in T_j} \cos(\mathbf{e}(t_k^{(i)}), \mathbf{e}(t_\ell^{(j)}))
\end{equation}

where $\mathbf{e}(\cdot)$ represents the embedding function and $\cos(\cdot, \cdot)$ denotes cosine similarity.

The bidirectional semantic similarity between rubrics $R_i$ and $R_j$ is then computed as:
\begin{equation}
\text{Sim}_{\text{semantic}}(R_i, R_j) = \frac{1}{m_i + m_j} \left( \sum_{k=1}^{m_i} \text{sim}_{\text{best}}(t_k^{(i)}, T_j) + \sum_{\ell=1}^{m_j} \text{sim}_{\text{best}}(t_\ell^{(j)}, T_i) \right)
\end{equation}

This formulation ensures that both the coverage of requirements from $R_i$ in $R_j$ and vice versa are considered, providing a balanced measure of semantic overlap.

\subsubsection{Structural Consistency Measurement}

Structural consistency evaluates the architectural similarity between rubrics, focusing on hierarchical organization rather than content semantics. We define a comprehensive set of structural features for each rubric:

\begin{itemize}
    \item \textbf{Maximum depth} ($d$): The deepest level in the rubric hierarchy
    \item \textbf{Total items} ($N$): The total number of rubric items across all levels
    \item \textbf{Weight distribution} ($W$): The frequency distribution of importance weights
    \item \textbf{Leaf ratio} ($r$): The proportion of leaf nodes to total nodes
\end{itemize}

The structural similarity between rubrics $R_i$ and $R_j$ combines multiple normalized metrics:

\begin{equation}
\text{Sim}_{\text{depth}}(R_i, R_j) = 1 - \frac{|d_i - d_j|}{\max(d_i, d_j, 1)}
\end{equation}

\begin{equation}
\text{Sim}_{\text{items}}(R_i, R_j) = 1 - \frac{|N_i - N_j|}{\max(N_i, N_j, 1)}
\end{equation}

For weight distribution similarity, we compute the overlap between normalized probability distributions:
\begin{equation}
\text{Sim}_{\text{weights}}(R_i, R_j) = \sum_{w \in W_i \cup W_j} \min(P_i(w), P_j(w))
\end{equation}

where $P_i(w)$ represents the normalized frequency of weight $w$ in rubric $R_i$.

The overall structural similarity is computed as:
\begin{equation}
\text{Sim}_{\text{structural}}(R_i, R_j) = \frac{1}{3}\left(\text{Sim}_{\text{depth}}(R_i, R_j) + \text{Sim}_{\text{items}}(R_i, R_j) + \text{Sim}_{\text{weights}}(R_i, R_j)\right)
\end{equation}

\subsection{Overall Reliability Score}

The overall reliability score aggregates consistency measures across all model pairs. For $n$ models, we compute $\binom{n}{2}$ pairwise comparisons and derive summary statistics:

\begin{equation}
\text{Score}_{\text{reliability}} = \frac{1}{\binom{n}{2}} \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \text{Sim}_{\text{semantic}}(R_i, R_j)
\end{equation}

Additionally, we compute the standard deviation of pairwise similarities to measure consistency variability:
\begin{equation}
\sigma_{\text{consistency}} = \sqrt{\frac{1}{\binom{n}{2}} \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \left(\text{Sim}_{\text{semantic}}(R_i, R_j) - \text{Score}_{\text{reliability}}\right)^2}
\end{equation}

\subsection{Reliability Interpretation}

The reliability assessment framework provides interpretable quality thresholds:
\begin{itemize}
    \item $\text{Score}_{\text{reliability}} \geq 0.8$: Excellent reliability
    \item $0.6 \leq \text{Score}_{\text{reliability}} < 0.8$: Good reliability  
    \item $0.4 \leq \text{Score}_{\text{reliability}} < 0.6$: Moderate reliability
    \item $\text{Score}_{\text{reliability}} < 0.4$: Low reliability
\end{itemize}

This assessment methodology enables systematic evaluation of rubric quality and provides quantitative evidence for the trustworthiness of automatically generated evaluation frameworks. The multi-dimensional approach ensures that both content validity and structural coherence are adequately captured in the reliability measure.
