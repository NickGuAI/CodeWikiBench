### instructions

If mode is `plan`:
-   Review the provided @task under given @context, write @analysis for the rootcause of the problem, and @fix_plan to resolve the problem. Add all relevant files to the @related_files section. Update this document at the end.

If mode is `execute`:
-   If the @fix_plan is not provided, ask the user for confirmation.
-   Execute the @fix_plan.

### context

### task

- Follow the OpenAI Cookbook instructions for [running GPT-OSS locally with Ollama](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama) so that `codebenchmark rubrics --model gpt-oss:20b` succeeds without placeholder outputs.
- Investigate why `codebenchmark rubrics --adapter deepwiki --repo electron --model gpt-oss:20b --use-tools` still yields `gpt-oss:20b_raw_output.txt` instead of a rubric JSON, fix the root cause, and keep rerunning the command until the saved output resembles `examples/OpenHands/rubrics/kimi-k2-instruct.json`.

It supports tool calls. fix code in /src/

### success_criteria

- Running `codebenchmark rubrics --adapter deepwiki --repo electron --model gpt-oss:20b --use-tools` produces `data/electron/rubrics/gpt-oss:20b.json` that matches the hierarchical rubric structure demonstrated in `examples/OpenHands/rubrics/kimi-k2-instruct.json` (no placeholder `TODO`s or raw dumps).

### analysis

- `codebenchmark rubrics` always enters `rubrics_generator.generate_rubrics.run`, which instantiates a `pydantic_ai.Agent` wired to `OpenAIChatModel` (`src/rubrics_generator/generate_rubrics.py`). That agent assumes OpenAI’s hosted Responses stack and never implements the chat-completions loop from the cookbook (send tool schema, watch `tool_calls`, run the tool, feed the full reasoning trace back). When `--model gpt-oss:20b` is passed, we still go through that path, so the Ollama-backed GPT-OSS run cannot call `docs_navigator` and the model falls back to placeholder responses.
- We also ship no logic that configures/speaks to the local Ollama endpoint spelled out in the cookbook (install/pull model, `http://localhost:11434/v1`, API key `ollama`), so `codebenchmark/cli.py` cannot route GPT-OSS requests to the proper runner even if the user followed the cookbook manually.

### fix_plan

- Expand `README.md` with the cookbook prerequisites (Ollama install, `ollama pull gpt-oss:20b`, start the daemon, set `BASE_URL`/`API_KEY`) plus an example `codebenchmark rubrics --model gpt-oss:20b` invocation so contributors know how to prep their machines.
- Extend `src/llm_proxy.py` with an Ollama-aware chat client that uses `AsyncOpenAI` against `http://localhost:11434/v1` and exposes a helper that runs the cookbook-described loop: send tool definitions, inspect `tool_calls`, execute `docs_navigator`, append the model’s reasoning back into the next `messages` payload, and stop once a final assistant text chunk arrives.
- Update `src/rubrics_generator/generate_rubrics.py` (and the glue in `src/codebenchmark/cli.py` if needed) to detect GPT-OSS models, swap to the new Ollama runner when `use_tools` is true, keep the existing `pydantic_ai.Agent` path for other providers, and ensure the resulting rubrics files + visualization flow stay unchanged.

### related_files

- README.md
- src/codebenchmark/cli.py
- src/rubrics_generator/generate_rubrics.py
- src/llm_proxy.py

### status

- Added README instructions for preparing Ollama+GPT-OSS, new GPT-OSS tool runner helpers in `src/llm_proxy.py`, and routing in `src/rubrics_generator/generate_rubrics.py` so `codebenchmark rubrics --model gpt-oss:20b` calls the docs navigator via the cookbook chat loop. Verified with `python -m py_compile src/llm_proxy.py src/rubrics_generator/generate_rubrics.py`.
