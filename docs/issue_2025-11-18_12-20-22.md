### instructions

If mode is `plan`:
-   Review the provided @task under given @context, write @analysis for the rootcause of the problem, and @fix_plan to resolve the problem. Add all relevant files to the @related_files section. Update this document at the end.

If mode is `execute`:
-   If the @fix_plan is not provided, ask the user for confirmation.
-   Execute the @fix_plan.

### context

README.md expects teams to download and parse the "official" docs into `data/<repo>/original`, but many benchmarking runs only have alternate sources (`deepwiki`, `codewiki`, etc.). We need a single note that clarifies how the rubrics generation and evaluation logic behaves when `original/` is absent and how to point the pipelines at whichever parsed docs actually exist.

### task

Explain how the rubrics and evaluation algorithms operate end to end, with an emphasis on what happens when `data/<repo>/original` is missing. Include a Mermaid diagram capturing the flow.

### analysis

- Rubrics generation (`src/run_rubrics_pipeline.sh` → `rubrics_generator/generate_rubrics.py`) assumes parsed docs live under `data/<repo>/<docs_source>`. It now exposes `--docs-source` (default `original`) so the operator can redirect the workflow whenever the `original` folder hasn’t been populated. Internally the Python script validates that `docs_tree.json` exists inside that folder before running the LLM agent.
- Evaluation (`src/run_evaluation_pipeline.sh` → `judge/judge.py`) already lets you pick any parsed docs folder via `--reference`. The script then loads the rubrics (combined or per-model) and runs the binary scoring agent only against the leaf requirements. If `original` is absent, you just set `--reference deepwiki` (or similar) and the judge will walk those docs as long as they contain `docs_tree.json` + `structured_docs.json`.
- The README instructions focus on pulling official docs for determinism, but the tooling itself works with any parsed folder. The key constraint is that every parsed folder must maintain the canonical outputs (`docs_tree.json` + `structured_docs.json`) so the `docs_navigator` tool can traverse files.
- Both pipelines follow the same pattern: choose source folder → confirm tree exists → run per-model agents → combine results → optional visualization.
- When `original/` is missing and you forget to pass `--docs-source`/`--reference`, the scripts stop immediately with a "Documentation tree not found" error. Passing the correct folder name (e.g., `codewiki`) resolves the issue without modifying any other logic.

### fix_plan

Document the workflow, calling out the `--docs-source` and `--reference` switches, the shared requirement for `docs_tree.json`, and the rubrics/evaluation algorithms. Provide a Mermaid diagram showing how different doc sources feed into the pipelines.

### related_files

- README.md
- src/run_rubrics_pipeline.sh
- src/rubrics_generator/generate_rubrics.py
- src/run_evaluation_pipeline.sh
- src/judge/judge.py

### execution

**Rubrics pipeline.** `run_rubrics_pipeline.sh` orchestrates the multi-model run. After validating `data/<repo>/<docs_source>/docs_tree.json`, it loops through each requested LLM model, calling `rubrics_generator/generate_rubrics.py`. That script loads the tree, instantiates the rubric agent (optionally with the `docs_navigator` tool), generates hierarchical criteria with weights + doc references, and writes them to `data/<repo>/rubrics/<model>.json`. The pipeline can then combine the results with `combine_rubrics.py` and optionally render the hierarchy.

**Algorithm walkthrough.** The pipeline isn't just file plumbing—it enforces a deterministic sequence for every model:

1. **Docs source detection (sh lines 24-53).** The shell script looks under `data/<repo>/` for any folder containing `docs_tree.json`. Preference order is `codewiki → deepwiki → original`, but if none exist it scans all folders alphabetically. This makes CodeWiki/DeepWiki drops first-class citizens.
2. **Model fan-out (sh lines 170-219).** Once a docs source is known, the script splits the `--models` list and runs `generate_rubrics.py` independently for each model so failures are isolated and outputs stay per-model.
3. **Agent execution (py lines 15-200).** `generate_rubrics.py` repeats the docs-source detection if no `--docs-source` flag was passed down, loads the tree, prints the chosen folder (useful when switching between CodeWiki/DeepWiki), and runs the Pydantic-AI agent. When `--use-tools` is set, the agent gains the `docs_navigator` tool to look up arbitrary leaves from `structured_docs.json` while assembling rubric text.
4. **Combination + metadata (sh lines 222-320, py combine script).** After per-model runs complete, `combine_rubrics.py` merges them (LLM-powered semantic merge first, deterministic fallback otherwise) and records statistics such as total nodes and depth, which later visualizers consume.
5. **Visualization/export.** Optional but the script surfaces the combined rubrics path and, if `--visualize` is set, renders the hierarchy. Even without that flag, the tail of the shell script enumerates the generated JSONs so operators know which per-model/combined files exist.

**How DeepWiki/CodeWiki affect execution.** Both sources ultimately produce the same pair of files, so the only difference is the folder name. Auto-detection means:

- When only CodeWiki docs exist, the script automatically chooses `codewiki/` and records that in log output; there is no dependency on `original/` being present.
- When DeepWiki and CodeWiki both exist, you can either let the preference order pick CodeWiki or pass `--docs-source deepwiki` to force DeepWiki.
- When CodeWiki is missing (common for internal repos) but DeepWiki is present, detection picks `deepwiki/` and everything else is unchanged.

**Call trace and math for rubric construction.** `run_rubrics_pipeline.sh` (lines 24-219) shells out to `python rubrics_generator/generate_rubrics.py` per model. Inside that module:

1. `detect_docs_source()` (src/rubrics_generator/generate_rubrics.py:9-25) walks `data/<repo>/<candidate>/docs_tree.json` and returns the first hit.
2. `run()` (same file:116-205) loads the chosen tree, builds a prompt that inlines the JSON, and instantiates a `pydantic_ai.Agent`. When `--use-tools` is set the agent receives the `docs_navigator` tool so it can fetch leaf files on demand.
3. `Agent.run()` produces raw text describing rubric nodes. The script then slices the first `[` ... last `]` span, runs `json.loads`, and writes `rubrics/<model>.json`. Each rubric item already contains the weight (integer 1–3) because the system prompt enforces that schema.
4. `visualize_rubrics()` renders a quick HTML/PNG tree, but more importantly `run_rubrics_pipeline.sh` returns the per-model JSON path to the caller so later stages can combine it.

The weights that show up in each JSON file are later treated as coefficients when aggregating evaluations (`calculate_scores_bottom_up` multiplies each child score by its weight before averaging). No additional math occurs during generation beyond the LLM adhering to `{1,2,3}` for importance.

**Combination internals.** After every requested model finishes, `run_rubrics_pipeline.sh` invokes `combine_rubrics.py` which performs:

1. `load_rubrics_files()` (src/rubrics_generator/combine_rubrics.py:145-185) globbing `rubrics/*.json` (excluding combined outputs) and loading each rubric list into memory.
2. `semantic_combine_rubrics()` (lines 22-143) builds a prompt that enumerates every tree under `rubrics_set_i`, calls the Anthropic backend named in `config.MODEL`, and expects a JSON dict with a `rubrics` array. The LLM instructions explicitly say “merge requirements when semantic similarity ≥70%.”
3. Failures (transport or parse) fall back to `fallback_simple_merge()` (lines 119-142), which concatenates all nodes and deduplicates by lowercase `name`.
4. `calculate_rubrics_statistics()` (lines 202-249) walks the merged tree recursively, counts nodes, and computes the weighted averages/depths shown in the CLI summary.

Because the combination output is itself JSON at `rubrics/combined_rubrics.json`, subsequent evaluation runs can be fed directly via `--rubrics-file` without any renaming.

**IEEE-style algorithm for `generate_rubrics.py`.** Condensed pseudocode for the async `run()` function illustrates how the rubric tree is synthesized:

```
Algorithm 1 RubricTreeSynthesis(args)
Input: repo name r, optional docs source d_opt, model identifier m, use-tools flag τ
Output: rubrics/<sanitized(m)>.json or *_raw_output.txt on failure
1: base ← config.get_data_path(r)
2: d ← d_opt if provided else detect_docs_source(base)
3: path ← base / d ; tree_file ← path / "docs_tree.json"
4: if tree_file ∉ files then raise FileNotFoundError
5: out_dir ← base / "rubrics" ; create if missing
6: model_tag ← m with '/' → '_' (default "default")
7: if out_dir/model_tag.json exists then return (already generated)
8: docs_tree ← json.load(tree_file)
9: prompt ← f("Given the docs tree: {json.dumps(docs_tree)}")
10: if τ then tools ← {docs_navigator} and system_prompt ← SYSTEM_PROMPT else tools ← ∅ and system_prompt ← SYSTEM_PROMPT_WO_TOOLS
11: agent ← Agent(model=get_llm(m), deps=AgentDeps, tools=tools, system_prompt)
12: deps ← AgentDeps(path)
13: final_output ← await agent.run(prompt, deps)
14: span ← substring of final_output between first '[' and last ']'
15: if span decodes via json.loads then rubrics ← parsed list else write raw text to out_dir/model_tag_raw_output.txt and return
16: write rubrics to out_dir/model_tag.json (indent=2)
17: visualize_rubrics(out_dir/model_tag.json)
18: return success marker
```

Steps 1–9 provide deterministic context to the LLM, steps 10–14 configure and execute the agent, and steps 15–18 enforce strict JSON parsing before persisting the hierarchy. The only stochastic component is the LLM response; all other operations are pure functions of the inputs.

```latex
\begin{tikzpicture}[>=Stealth,
    node distance=1.8cm,
    stage/.style={draw, rounded corners, align=center, font=\small, text width=6.3cm, minimum height=1.2cm, fill=blue!6},
    comb/.style={draw, rounded corners, align=center, font=\small, text width=6.3cm, minimum height=1.2cm, fill=teal!10}]
    \node[stage] (P0) {run\_rubrics\_pipeline.sh\\validate docs source and expand --models};
    \node[stage, below=of P0] (P1) {detect\_docs\_source()\\scan data/<repo>/* for docs\_tree.json};
    \node[stage, below=of P1] (P2) {generate\_rubrics.run()\\load docs tree and craft prompt};
    \node[stage, below=of P2] (P3) {Agent.run()\\SYSTEM\_PROMPT + optional docs\_navigator};
    \node[stage, below=of P3] (P4) {json.loads(model output)\\extract hierarchy with weights \{1,2,3\}};
    \node[stage, below=of P4] (P5) {rubrics/<model>.json\\visualize\_rubrics() artifact};

    \node[comb, below=1.6cm of P5] (C0) {load\_rubrics\_files()\\glob per-model JSON files};
    \node[comb, below=of C0] (C1) {semantic\_combine\_rubrics()\\Anthropic merge of \(\geq 70\%\) similar nodes};
    \node[comb, below=of C1] (C3) {calculate\_rubrics\_statistics()\\count nodes, depth, weight histogram};
    \node[comb, below=of C3] (C4) {rubrics/combined\_rubrics.json\\write metadata + combined tree};
    \node[comb, right=4cm of C1, dashed] (C2) {fallback\_simple\_merge()\\name-based deduplication};

    \draw[->] (P0) -- (P1);
    \draw[->] (P1) -- (P2);
    \draw[->] (P2) -- (P3);
    \draw[->] (P3) -- (P4);
    \draw[->] (P4) -- (P5);
    \draw[->] (P5) -- (C0);
    \draw[->] (C0) -- (C1);
    \draw[->] (C1) -- (C3);
    \draw[->] (C3) -- (C4);
    \draw[->, dashed] (C1) -- node[above, font=\scriptsize]{if LLM fails} (C2);
\end{tikzpicture}
```

**Evaluation pipeline.** `run_evaluation_pipeline.sh` performs the complementary loop. It targets whatever folder you pass via `--reference` (default `original`, but often `deepwiki` or `codewiki`). For every model, it runs `judge/judge.py`, which loads the rubrics (usually `rubrics/combined_rubrics.json`), collects every leaf requirement, and queries the LLM agent to assign binary scores with reasoning/evidence. Scores propagate upward by weight to produce a fully scored rubric tree per model. Finally, `combine_evaluations.py` fuses the per-model files (average, majority vote, etc.), and `visualize_evaluation.py` can summarize or export the combined results.

**When `original/` is missing.** Nothing in either algorithm actually relies on the folder being named `original`; they only require a parsed docs directory that contains the canonical outputs. If `data/<repo>/original/docs_tree.json` is missing, pass the relevant folder explicitly (e.g., `--docs-source deepwiki` when generating rubrics, `--reference deepwiki` when judging). The scripts will validate that folder, use its `docs_tree.json` for navigation, and write outputs exactly as before.

```latex
\begin{tikzpicture}[>=Stealth,
    node distance=1.7cm,
    stage/.style={draw, rounded corners, align=center, font=\small, text width=6.1cm, minimum height=1.1cm, fill=gray!12}]
    \node[stage] (S) {Docs sources\\original / deepwiki / codewiki dumps};
    \node[stage, below=of S, fill=orange!15] (P) {Parsed docs folder\\docs\_tree.json + structured\_docs.json};
    \node[stage, below=of P, fill=blue!8] (R) {run\_rubrics\_pipeline.sh\\pass --docs-source if needed};
    \node[stage, below=of R, fill=blue!8] (G) {generate\_rubrics.py\\per-model agents create weighted hierarchies};
    \node[stage, below=of G, fill=blue!8] (RC) {combine\_rubrics.py\\semantic merge + stats};
    \node[stage, below=of RC, fill=green!20] (RUB) {Rubrics store\\per-model JSON + combined file};
    \node[stage, below=of RUB, fill=purple!15] (E) {run\_evaluation\_pipeline.sh\\set --reference to docs source};
    \node[stage, below=of E, fill=purple!15] (J) {judge/judge.py\\binary scoring of leaf requirements};
    \node[stage, below=of J, fill=purple!15] (CE) {combine\_evaluations.py\\aggregate model results};
    \node[stage, below=of CE, fill=green!20] (COMB) {combined evaluation JSON\\weighted totals + reasoning};
    \node[stage, below=of COMB, fill=yellow!20] (VIS) {visualize\_evaluation.py\\human-friendly reports};

    \draw[->] (S) -- node[right, font=\scriptsize]{tree + structured docs} (P);
    \draw[->] (P) -- node[right, font=\scriptsize]{--docs-source} (R);
    \draw[->] (R) -- node[right, font=\scriptsize]{per model fan-out} (G);
    \draw[->] (G) -- node[right, font=\scriptsize]{rubrics per model} (RC);
    \draw[->] (RC) -- node[right, font=\scriptsize]{combined rubrics} (RUB);
    \draw[->] (RUB) -- node[right, font=\scriptsize]{--reference folder} (E);
    \draw[->] (E) -- node[right, font=\scriptsize]{per model eval} (J);
    \draw[->] (J) -- node[right, font=\scriptsize]{leaf scores} (CE);
    \draw[->] (CE) -- node[right, font=\scriptsize]{aggregate metrics} (COMB);
    \draw[->] (COMB) -- node[right, font=\scriptsize]{optional summaries} (VIS);
\end{tikzpicture}
```

With this setup you only have to ensure that at least one folder under `data/<repo>/` contains parsed documentation. Point both pipelines at that folder and the rubrics/evaluation algorithms behave identically to the README’s "original" workflow.
