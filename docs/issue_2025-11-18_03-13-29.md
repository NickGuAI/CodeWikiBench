### instructions

If mode is `plan`:
-   Review the provided @task under given @context, write @analysis for the rootcause of the problem, and @fix_plan to resolve the problem. Add all relevant files to the @related_files section. Update this document at the end.

If mode is `execute`:
-   If the @fix_plan is not provided, ask the user for confirmation.
-   Execute the @fix_plan.

### context

### task

Explain how rubric is generated from where, and how evaluation is run. check examples for example content.

### analysis

The rubric generation and evaluation processes are orchestrated by shell scripts and implemented using Python scripts that leverage Large Language Models (LLMs).

**Rubric Generation:**

1.  **Initiation:** The process starts with the `run_rubrics_pipeline.sh` script.
2.  **Core Logic:** This script calls `src/rubrics_generator/generate_rubrics.py`.
3.  **Input:** `generate_rubrics.py` takes a repository's documentation, which has been pre-processed into a `docs_tree.json` (structure) and `structured_docs.json` (content).
4.  **AI-Powered Generation:** It uses a Large Language Model (LLM) agent to analyze the documentation and generate a hierarchical set of rubrics. The agent is given a detailed system prompt explaining its task and the desired JSON output format.
5.  **Tool-Assisted Analysis:** The agent can use a `docs_navigator` tool to read specific parts of the documentation, allowing it to "browse" the docs to get more context. This tool reads the documentation content from `structured_docs.json` based on the paths provided.
6.  **Output:** The output is a JSON file containing the hierarchical rubrics, where each rubric has a name, description, weight (importance), and, for the most specific "leaf" rubrics, references to the documentation.
7.  **Combination:** The `run_rubrics_pipeline.sh` script then combines rubrics from different sources (if any) using `combine_rubrics.py`.

**Evaluation:**

1.  **Initiation:** The evaluation process is started by the `run_evaluation_pipeline.sh` script.
2.  **Core Logic:** This script calls `src/judge/judge.py`.
3.  **Input:** `judge.py` takes the generated rubrics and a set of documentation to be evaluated.
4.  **Leaf-level Evaluation:** The script focuses on the "leaf" rubrics, which are the most specific criteria.
5.  **AI-Powered Judgment:** For each leaf rubric, it uses an LLM agent to determine if the documentation meets that criterion. The agent is guided by a system prompt that defines its role as a documentation evaluation expert.
6.  **Binary Score:** The agent provides a binary score (1 for documented, 0 for not documented), along with reasoning and evidence from the documentation.
7.  **Tool-Assisted Verification:** Similar to rubric generation, the agent can use the `docs_navigator` tool to search for evidence within the documentation.
8.  **Score Aggregation:** After evaluating all leaf rubrics, the script calculates scores for higher-level rubrics by taking a weighted average of the scores of their children. This provides a comprehensive, hierarchical view of the documentation's quality.
9.  **Output:** The final output is a JSON file containing the scored rubrics, which shows the evaluation results for every level of the hierarchy.

### fix_plan

- Update this markdown file with the explanation of the rubric generation and evaluation process.

### related_files