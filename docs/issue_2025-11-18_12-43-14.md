### instructions

If mode is `plan`:
-   Review the provided @task under given @context, write @analysis for the rootcause of the problem, and @fix_plan to resolve the problem. Add all relevant files to the @related_files section. Update this document at the end.

If mode is `execute`:
-   If the @fix_plan is not provided, ask the user for confirmation.
-   Execute the @fix_plan.

### context

The module within src/ supports the following
1. download wikis (deepwiki)
2. parse wikis (deepwiki, codewiki)
3. run rubrics generation (deepwiki, codewiki)

### task

<log>(base) yugu@Mac src % python docs_parser/parse_generated_docs.py --input-dir ../data/electron/deepwiki/docs --output-dir ../data/electron/deepwiki
  Traceback (most recent call last):
    File "/Users/yugu/Desktop/gehirn/CodeWikiBench/src/docs_parser/parse_generated_docs.py", line 3, in <module>
      import markdown_to_json
  ModuleNotFoundError: No module named 'markdown_to_json'</log> failed. I think this project has many issues that makes it not production ready and is
  just a fucking toy. 
  
1. switch to use uv
2. create a command entry point to run the benchmarks. i.e 
   1. `codebenchmark download --adapter deepwiki --url https://deepwiki.com/electron/electron --output-dir ../data/electron/deepwiki/docs`
   2. `codebenchmark parse --adapter deepwiki --repo electron`
   3. `codebenchmark parse --adapter codewiki --repo electron`
   4. `codebenchmark rubrics --adapter deepwiki --repo electron --model gpt-4o`
   5. `codebenchmark rubrics --adapter codewiki --repo electron --model gpt-4o --visualize`
   6. `codebenchmark eval --adapter deepwiki --repo electron --model gpt-4o (--visualize)`

### analysis

- `src/docs_parser/parse_generated_docs.py:3` imports `markdown_to_json`, yet we do not provide a reproducible environment definition (no `pyproject.toml`, lock file, or installer instructions), so anyone who forgets to `pip install -r requirements.txt` hits the `ModuleNotFoundError` recorded in the task log.
- The repo is still treated like scripts glued together—`requirements.txt` is the sole dependency spec, there is no pinned Python version, and nothing integrates with uv—so the tooling feels like a toy rather than an installable benchmark.
- All user workflows (download, parse, rubrics, eval) live in isolated scripts (`docs_parser/*.py`, the bash pipelines, `rubrics_generator/generate_rubrics.py`, `judge/judge.py`). Without a unified Python package there is no way to register a `codebenchmark` entry point or share logic (docs-source detection, data paths, input validation) between commands.

### fix_plan

1. **Adopt uv for dependency management**
   - Author a `pyproject.toml` that declares the project metadata, Python requirement, and dependency list (including `markdown_to_json`), then generate `uv.lock`. Keep `requirements.txt` as a generated artifact or pointer and document `uv sync` / `uv run` workflow in `README.md`.
2. **Make download/parse routines importable**
   - Refactor `src/docs_parser/crawl_deepwiki_docs.py` and `src/docs_parser/parse_generated_docs.py` so their logic lives in functions that accept the CLI arguments and return status objects, making them callable from both scripts and the new CLI. Surface adapter-specific handlers (`deepwiki`, `codewiki`) with clear validation errors.
3. **Implement the `codebenchmark` CLI**
   - Create `src/codebenchmark/__init__.py` and `src/codebenchmark/cli.py` (likely using `click` given it is already a dependency) and register `[project.scripts] codebenchmark = "codebenchmark.cli:app"` in `pyproject.toml`.
   - Add subcommands matching the user’s expectations:
     - `download`: dispatch to the deepwiki crawler (future adapters can plug in) and ensure output directories are created.
     - `parse`: run the deepwiki/codewiki parser helpers, defaulting to `data/<repo>/<adapter>` locations when flags are omitted.
     - `rubrics`: move the orchestration currently inside `src/run_rubrics_pipeline.sh` into Python so we can drive `rubrics_generator/generate_rubrics.py` for each model, handle combination/visualization flags, and share docs-source detection logic.
     - `eval`: replace `src/run_evaluation_pipeline.sh` with a Python implementation that calls into `src/judge/judge.py`, handles batching/retries, and optionally visualizes results.
4. **Documentation & validation**
   - Update `README.md` to demonstrate installing via uv and running the new CLI commands.
   - Ensure `codebenchmark --help` (or a similar smoke test) runs in CI so missing dependencies or entry point regressions are caught immediately.

### related_files

- requirements.txt
- pyproject.toml
- README.md
- src/docs_parser/crawl_deepwiki_docs.py
- src/docs_parser/parse_generated_docs.py
- src/codebenchmark/__init__.py
- src/codebenchmark/cli.py
- src/rubrics_generator/generate_rubrics.py
- src/run_rubrics_pipeline.sh
- src/judge/judge.py
- src/run_evaluation_pipeline.sh
